Student Success Predictor
A simple ML service I built to learn the full deployment pipeline—from training a model to exposing it through an API. It predicts whether a student will pass an exam based on study habits and attendance.

The model itself isn't sophisticated (just logistic regression), but the whole point was figuring out how to package everything with FastAPI and Docker so it actually runs somewhere other than my laptop.

Stack: Python 3.10 | FastAPI | scikit-learn | Docker

What It Does
You give it data about a student:

How many hours they studied

Their attendance rate

Their average grade so far

How many assignments they completed

How much they sleep

The API predicts whether they'll pass (1) or fail (0), plus gives you the confidence as a probability between 0 and 1.

That's it. Nothing fancy. But it's the full pipeline—synthetic data, training, serialization, REST API, containerization. All the things you actually need in real work.

Why I Built This
I was tired of building models in Jupyter notebooks that never left my laptop. Every course project ended with a .ipynb file that had 500 cells and no way to actually use it. So I decided to force myself to build something deployable from scratch.

I started with a simple question: "Can I predict if someone passes an exam from behavioral data?" The answer doesn't really matter—what matters is learning how to go from training a model to having an API that another program can call.

Running Locally
Setup
bash
# Create the environment
conda create -n ml python=3.10
conda activate ml

# Install dependencies (seriously, do this first)
pip install -r requirements.txt

# Train the model (generates synthetic data + saves the trained model)
python src/train_model.py

# Start the API
uvicorn src.api:app --reload
That's it. The API is now running at http://127.0.0.1:8000.

Go to http://127.0.0.1:8000/docs to see the interactive API docs. This is where you can test the /predict endpoint without writing any code.

What Actually Happens When You Run This
train_model.py generates 500 fake student records, trains a logistic regression model on them, and saves it to models/student_success_model.joblib. This takes like 2 seconds.

The FastAPI server loads that model into memory and waits for requests. When you send a POST request with student data, it runs the prediction and sends back the result.

Using the API
Health Check
Just to make sure it's alive:

bash
curl http://127.0.0.1:8000/
You'll get back:

json
{"message": "API is running"}
Make a Prediction
bash
curl -X POST http://127.0.0.1:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "hours_studied": 5,
    "attendance_rate": 0.9,
    "past_grade_avg": 7.5,
    "assignments_completed": 12,
    "sleep_hours": 7
  }'
Response:

json
{
  "prediction": 1,
  "pass_probability": 0.87
}
prediction is the final call (pass/fail). pass_probability is how confident the model is.

The thing is, the model was trained on synthetic data, so these probabilities are totally made up. But the mechanics of what's happening here—data validation, feature scaling, model inference, JSON response—that's all real and transfers to actual datasets.

Docker (The Real Test)
This is where you know if your code actually works or if it just works on your machine.

Build It
bash
docker build -t student-success-api .
First time this runs, it downloads the Python base image and installs dependencies. Takes a minute. After that, rebuilds are fast.

Run It
bash
docker run -p 8000:8000 student-success-api
Same API, running in a container. Visit http://127.0.0.1:8000/docs again and test it.

The magic here is that you could send this Dockerfile to someone else and they'd get exactly the same environment. No "works on my machine" nonsense. This is why Docker matters for ML.

Project Structure
text
student-success-api/
├── data/                              # (optional, for real datasets later)
├── models/
│   └── student_success_model.joblib   # the trained model (generated by train_model.py)
├── src/
│   ├── train_model.py                 # generates synthetic data + trains model
│   └── api.py                         # FastAPI server
├── requirements.txt
├── Dockerfile
├── .dockerignore
└── README.md
The important files:

train_model.py is where the ML happens—generates fake data, trains the model, saves it

api.py is the server—loads the model and handles requests

Everything in src/ gets copied into the Docker image

What I Learned Building This
Feature scaling matters. The first version didn't use StandardScaler and the model predictions were garbage. Took me an embarrassing amount of time to figure that out.

Joblib is simple and it works. I could've used pickle, but joblib is designed for scikit-learn models. Just works.

FastAPI is actually enjoyable to use. I expected to hate it, but the auto-generated docs and built-in validation are legitimately helpful.

Docker adds complexity upfront but saves time later. Spent 30 minutes getting the Dockerfile right, but now deployment is boring (which is good).

Synthetic data is good for learning but will bite you later. My model gets perfect accuracy on this fake data because I generated it. With real data? Probably 65% if I'm lucky. This is important to keep in mind.

Next Steps (If You Want to Actually Use This)
Replace the synthetic data. Find a real student dataset. Things will break immediately because real data is messier than what I generated. This is where you learn.

Try a different model. Logistic regression is a baseline. Try RandomForest or XGBoost. The API stays the same—you just swap out the model inside train_model.py.

Add a database. Right now everything's in memory. Add PostgreSQL to log predictions, store model versions, track accuracy over time.

Add proper error handling. Send me invalid data right now and the API will probably crash. Handle edge cases.

Deploy it somewhere. Render, Railway, AWS—pick one and actually deploy this. You'll hit networking problems, environment variable issues, and other things that don't happen locally.

Add authentication. Secure the API so not everyone can call it. Learn about JWT or API keys.

Tech Stack (Why I Picked These)
Python 3.10: It's 2025, use 3.10 at minimum. 3.11+ is fine too.

scikit-learn: ML library that just works for small models. Not overkill like TensorFlow for a simple classifier.

FastAPI: Modern, fast, with built-in validation and docs. Way better than Flask for this.

joblib: Standard for saving scikit-learn models. One line to save, one line to load.

Docker: Container your code so it runs the same everywhere. Non-negotiable for deployment.

Uvicorn: ASGI server that runs FastAPI. Simple and reliable.

Common Issues
ModuleNotFoundError when running the API: You didn't activate the conda environment or didn't install requirements. Run pip install -r requirements.txt again.

Port 8000 already in use: Something else is running on that port. Either stop it or run the API on a different port: uvicorn src.api:app --reload --port 8001

Docker build fails with "python" not found: Make sure you have Python 3.10+ installed locally if building on your machine. The Dockerfile pulls it automatically though.

Model predictions seem random: Yeah, it's synthetic data. The model learned patterns that only exist in the fake data I generated. This is why you need real data.

License
MIT. Do whatever you want with this.